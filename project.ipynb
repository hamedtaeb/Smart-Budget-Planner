{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cac329a",
   "metadata": {},
   "source": [
    "## Spending Personality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "#import hdbscan\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset \n",
    "df = pd.read_csv(\"./data/data.csv\", sep='\\t')\n",
    "\n",
    "# Basic Info\n",
    "df.shape\n",
    "df.info()\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d2c6e",
   "metadata": {},
   "source": [
    "### Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe(include=\"number\").T.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick category peek\n",
    "for col in [\"Education\", \"Marital_Status\"]:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nValue counts for {col}:\")\n",
    "        print(df[col].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bdab4",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Categorical Columns\n",
    "df['Marital_Status'] = df['Marital_Status'].str.strip()\n",
    "df['Education'] = df['Education'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c030e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Wired Martial_Status Entries\n",
    "martial_map = {\n",
    "    'Alone': 'Single',\n",
    "    'Absurd': 'Single',\n",
    "    'YOLO': 'Single',\n",
    "}\n",
    "\n",
    "df['Marital_Status_norm'] = df['Marital_Status'].replace(martial_map)\n",
    "\n",
    "\n",
    "# Normalize 'Education' Entries\n",
    "edu_map = {\n",
    "    '2n Cycle': 'Graduate',\n",
    "    'Graduation': 'Graduate',\n",
    "    'Master': 'Post-Graduate',\n",
    "    'PhD': 'Post-Graduate',\n",
    "    'Basic': 'Basic'\n",
    "}\n",
    "\n",
    "df['Education_norm'] = df['Education'].replace(edu_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Data\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], errors='coerce', dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Missing Income \n",
    "df['Income']= pd.to_numeric(df['Income'], errors='coerce')\n",
    "group_median = df.groupby(['Education_norm', 'Marital_Status_norm'])['Income'].transform('median')\n",
    "df['Income'] = df['Income'].fillna(group_median)\n",
    "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
    "\n",
    "print(\"Remaining nulls in income:\", df['Income'].isnull().sum())\n",
    "df[['Education', 'Education_norm', 'Marital_Status', 'Marital_Status_norm']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5045f3d",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference date - Today for our age & tenure calculations\n",
    "REF_DATE = pd.Timestamp.today().normalize()\n",
    "\n",
    "# Calculate Customer Age\n",
    "df['Age'] = REF_DATE.year - df['Year_Birth']\n",
    "\n",
    "# Children QTY\n",
    "df['Children'] = df['Kidhome'] + df['Teenhome']\n",
    "\n",
    "# Customer Tenure  : Measures loyalty or relationship age\n",
    "df['Customer_For_Days'] = (REF_DATE - df['Dt_Customer']).dt.days\n",
    "df['Customer_For_years'] = (df['Customer_For_Days'] / 365.25).round(1)\n",
    "\n",
    "\n",
    "# Total Spending ( Sum of all Mnt* columns)\n",
    "mnt_cols = [col for col in df.columns if col.startswith('Mnt')]\n",
    "df['TotalMnt'] = df[mnt_cols].sum(axis=1)\n",
    "\n",
    "# Total Purchase \n",
    "purchase_cols = [col for col in df.columns if col.startswith('Num') and col.endswith('Purchases')]\n",
    "df['TotalPurchases'] = df[purchase_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total Purchase \n",
    "purchase_cols = [col for col in df.columns if col.startswith('Num') and col.endswith('Purchases')]\n",
    "df['TotalPurchases'] = df[purchase_cols].sum(axis=1)\n",
    "\n",
    "# Average Spend Per Purchase \n",
    "df[\"AvgMntPerPurchase\"] = np.where(df['TotalPurchases'] > 0,\n",
    "                                   df['TotalMnt'] / df['TotalPurchases'], \n",
    "                                   np.nan)\n",
    "\n",
    "# Replacing rows with 0 transactions to 1 as not possible to generate a spend with no txs\n",
    "df[\"TotalPurchases\"] = df[\"TotalPurchases\"].replace(\n",
    "    {\n",
    "        0 : 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e57e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANNEL SHARES\n",
    "for channel in ['Web', 'Catalog', 'Store']:\n",
    "    col = f'Num{channel}Purchases'\n",
    "    if col in df.columns:\n",
    "        df[f'{channel}PurchaseShare'] = np.where(df['TotalPurchases'] > 0,\n",
    "                                                 df[col] / df['TotalPurchases'],\n",
    "                                                 np.nan)\n",
    "        \n",
    "\n",
    "\n",
    "# Deal Purchase Rate\n",
    "df['DealPurchaseRate'] = np.where(df['TotalPurchases'] > 0,\n",
    "                                  df['NumDealsPurchases'] / df['TotalPurchases'],\n",
    "                                  np.nan)\n",
    "\n",
    "\n",
    "# Campaign Response Rate\n",
    "cmp_cols = [col for col in df.columns if col.startswith('AcceptedCmp')]\n",
    "df['CampaignsAccepted'] = df[cmp_cols].sum(axis=1)\n",
    "df['Responded'] = (df['Response'] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the changes \n",
    "df[['Age', 'Children', 'Customer_For_years', 'TotalMnt', 'TotalPurchases', \n",
    "    'AvgMntPerPurchase', 'WebPurchaseShare', 'CatalogPurchaseShare',\n",
    "    'StorePurchaseShare', 'DealPurchaseRate', 'CampaignsAccepted', 'Responded']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6a7d3",
   "metadata": {},
   "source": [
    "## Data Cleaning & Feature Engineering Summary\n",
    "\n",
    "### 1. Data Cleaning\n",
    "We started by inspecting the dataset for missing values, incorrect data types, and inconsistent categories.\n",
    "\n",
    "- **Missing Values:** Only the `Income` column had missing entries (~1%).  \n",
    "  → Imputed using the **median income** of each `(Education_norm, Marital_Status_norm)` group.  \n",
    "- **Categorical Normalization:**\n",
    "  - Mapped unusual `Marital_Status` values like `\"YOLO\"`, `\"Absurd\"`, and `\"Alone\"` to `\"Single\"`.\n",
    "  - Simplified `Education` levels by merging similar categories:\n",
    "    - `\"2n Cycle\"` and `\"Graduation\"` → `\"Graduate\"`.\n",
    "    - `\"Master\"` and `\"PhD\"` → `\"Postgraduate\"`.\n",
    "    - `\"Basic\"` kept as `\"Basic\"`.\n",
    "- **Date Parsing:** Converted `Dt_Customer` to proper datetime format for tenure calculations.\n",
    "\n",
    "### 2. Feature Engineering\n",
    "Created meaningful new variables to better represent customer behavior and demographics:\n",
    "\n",
    "| Feature | Description | Insight |\n",
    "|----------|--------------|----------|\n",
    "| **Age** | `current_year - Year_Birth` | Captures life stage and spending maturity. |\n",
    "| **Children** | `Kidhome + Teenhome` | Indicates family size and dependency load. |\n",
    "| **Customer_For_years** | Tenure calculated from `Dt_Customer`. | Reflects customer loyalty and relationship length. |\n",
    "| **TotalMnt** | Sum of all `Mnt*` (spending) columns. | Represents overall spending volume. |\n",
    "| **TotalPurchases** | Sum of all `Num*Purchases` columns. | Measures overall purchase activity. |\n",
    "| **AvgMntPerPurchase** | `TotalMnt / TotalPurchases`. | Shows average order value per transaction. |\n",
    "| **WebPurchaseShare**, **CatalogPurchaseShare**, **StorePurchaseShare** | Ratio of purchases per channel. | Highlights preferred shopping channels. |\n",
    "| **DealPurchaseRate** | `NumDealsPurchases / TotalPurchases`. | Indicates price sensitivity or bargain-hunting behavior. |\n",
    "| **CampaignsAccepted** | Sum of all `AcceptedCmp*` columns. | Shows how often the customer accepted previous campaigns. |\n",
    "| **Responded** | Binary flag from `Response` (1 if accepted last campaign). | Reflects latest engagement behavior. |\n",
    "\n",
    "### 3. Why These Features Matter\n",
    "These engineered features summarize customer spending habits and engagement patterns.  \n",
    "They help us:\n",
    "- Compare **planned vs. impulsive** shoppers (e.g., high deal rate vs. low deal rate).\n",
    "- Understand **loyalty** and **tenure** (longer customers may behave differently).\n",
    "- Detect **channel preferences** (online vs. in-store).\n",
    "- Prepare for building **spending personality profiles** (Saver, Splurger, Planner, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "The dataset is now cleaned, consistent, and enriched with features ready for exploratory analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b9af4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894611b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Summary\n",
    "display(df[['Income', 'Age', 'Children', 'TotalMnt', 'TotalPurchases',\n",
    "            'AvgMntPerPurchase', 'DealPurchaseRate', 'CampaignsAccepted',\n",
    "            'Customer_For_years']].describe().T.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check_zero_puchase  = df['TotalPurchases'] == 0\n",
    "print(\"Number of customers with zero purchases:\", Check_zero_puchase.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "def plot_hist(series, title, bins=30):\n",
    "    plt.figure()\n",
    "    series.dropna().plot(kind='hist', bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(series.name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "for col in ['Income', 'Age', 'TotalMnt', 'TotalPurchases', 'AvgMntPerPurchase']:\n",
    "    if col in df.columns:\n",
    "        plot_hist(df[col], f'{col} distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef501e",
   "metadata": {},
   "source": [
    "What the charts tell us\n",
    "1. **Income distribution**\n",
    "- Very right-skewed: most customers earn under $100K, but there are a few extreme outliers up to $600K+.\n",
    "- These outliers will heavily influence correlations and averages.\n",
    "\n",
    "2. **Age distribution**\n",
    "- Centered around 45–65 years, meaning this customer base is mostly middle-aged adults.\n",
    "- The one record showing Age >120 is clearly an outlier — likely a data entry error (e.g., wrong birth year). <br/>\n",
    "→ We might need to cap Age at 90 later.\n",
    "\n",
    "3. **TotalMnt (Total Spend)**\n",
    "- Also right-skewed: most people spend under $1,000, but a small fraction go up to $2,500+.\n",
    "- This indicates a few heavy buyers — possibly our “Splurgers” or “High-value” personalities later.\n",
    "\n",
    "4.**Total Purchases**\n",
    "- Ranges roughly 0–40, average ~15 — a wide range in engagement.\n",
    "- Suggests there are light shoppers and heavy repeat buyers — a strong behavioral differentiator.\n",
    "\n",
    "5. **AvgMntPerPurchase**\n",
    "- Extremely right-skewed — most purchases are small, but some individuals spend hundreds per purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship Check\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df['Income'], df['TotalMnt'], alpha=0.5)\n",
    "plt.title('Income vs Total Spend')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Total Spend(TotalMnt)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df['Age'], df['TotalMnt'], alpha=0.5)\n",
    "plt.title('Age vs Total Spend')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Total Spend(TotalMnt)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df['DealPurchaseRate'], df['TotalMnt'], alpha=0.5)\n",
    "plt.title('Deal Purchase Rate vs Total Spend')\n",
    "plt.xlabel('Deal Purchase Rate')\n",
    "plt.ylabel('Total Spend(TotalMnt)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357c32c",
   "metadata": {},
   "source": [
    "### Relationships**\n",
    "**Income vs Total Spend**\n",
    "- Clear positive relationship: higher income → higher spend, but with a wide vertical spread.<br/>\n",
    "→ Interpretation: income affects spending capacity, but not alone — other factors (deals, habits, family size) matter.\n",
    "\n",
    "**Age vs Total Spend**\n",
    "- No obvious linear trend — spending seems scattered across ages.<br/>\n",
    "→ Spending behavior is not strongly age-driven.\n",
    "\n",
    "**Deal Purchase Rate vs Total Spend**\n",
    "- Negative trend: as deal rate increases, total spend decreases.<br/>\n",
    "→ Bargain hunters tend to spend less overall — consistent with the “Saver” archetype we’ll define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Numeric Columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Correlation Matrix\n",
    "corr = df[numeric_cols].corr()\n",
    "\n",
    "# Focus on correlations with Spending Behavior\n",
    "target_vars =['TotalMnt', 'TotalPurchases']\n",
    "for target in target_vars:\n",
    "    if target in corr.columns:\n",
    "        print(f\"\\nTop correlations with {target}:\")\n",
    "        display(corr[target].sort_values(ascending=False).head(10))\n",
    "        display(corr[target].sort_values(ascending=True).head(10))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Correlation Heatmap\n",
    "key_cols = [\n",
    "    'Income', 'Age' , 'Children', 'Customer_For_years', 'TotalMnt', 'TotalPurchases',\n",
    "    'AvgMntPerPurchase', 'WebPurchaseShare', 'catalogPurchaseShare',\n",
    "    'StorePurchaseShare', 'DealPurchaseRate', 'CampaignsAccepted', 'Responded'\n",
    "]\n",
    "\n",
    "\n",
    "key_cols = [c for c in key_cols if c in df.columns]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(df[key_cols].corr(), cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar(label='Correlation Coefficient')\n",
    "plt.xticks(range(len(key_cols)), key_cols, rotation=90)\n",
    "plt.yticks(range(len(key_cols)), key_cols)\n",
    "plt.title(\"Correlation Matric(Key Spending Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0005db",
   "metadata": {},
   "source": [
    "## Correlation Analysis — Key Insights\n",
    "\n",
    "### Overview\n",
    "The correlation matrix and feature relationships reveal strong behavioral patterns that connect spending capacity, activity level, and shopping style.\n",
    "\n",
    "\n",
    "###  1. Income and Total Spending\n",
    "- **Strong positive correlation (~0.6–0.7)**  \n",
    "  Higher income customers consistently spend more overall.  \n",
    "  → *Income* is a key driver of spending capacity.\n",
    "\n",
    "\n",
    "###  2. Total Purchases and Deal Purchase Rate\n",
    "- **Moderate negative correlation (~–0.4)**  \n",
    "  Customers who rely more on discounts make fewer purchases overall.  \n",
    "  → Reflects *Saver* or *Deal-Seeker* behavior — they wait for promotions and buy selectively.\n",
    "\n",
    "\n",
    "###  3. Channel Behavior\n",
    "- **Store Purchase Share:** Negatively correlated with Total Purchases (~–0.3)  \n",
    "  → In-store shoppers purchase less frequently (more deliberate, planned).\n",
    "\n",
    "- **Web Purchase Share:** Positively correlated (likely +0.3)  \n",
    "  → Online shoppers tend to buy more often — possibly more impulsive.\n",
    "\n",
    "\n",
    "###  4. Family Impact\n",
    "- **Children / Kidhome:** Negatively correlated with Total Purchases (~–0.25 to –0.48)  \n",
    "  → More kids at home → tighter budgets and reduced discretionary spending.  \n",
    "  Indicates a cautious, *Saver-type* pattern.\n",
    "\n",
    "\n",
    "###  5. Age\n",
    "- Weak negative correlation with Total Purchases.  \n",
    "  → Spending doesn’t vary much by age; older customers are not necessarily higher spenders.\n",
    "\n",
    "\n",
    "### 6.  Marketing Engagement\n",
    "- **CampaignsAccepted & Responded:** Mild positive correlation with Total Spend (~+0.2).  \n",
    "  → Engaged customers spend slightly more and may represent loyal *Planner* types.\n",
    "\n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "| Behavior Dimension | Strong Indicators | Interpretation |\n",
    "|--------------------|------------------|----------------|\n",
    "| **Spending Capacity** | `Income`, `TotalMnt` | Financial ability to spend |\n",
    "| **Spending Activity** | `TotalPurchases`, `AvgMntPerPurchase` | Frequency and basket size |\n",
    "| **Impulse vs. Planning** | `DealPurchaseRate`, `WebPurchaseShare`, `StorePurchaseShare` | How spontaneous or deliberate the buyer is |\n",
    "| **Engagement & Responsiveness** | `CampaignsAccepted`, `Responded` | Brand interaction and marketing response |\n",
    "\n",
    "###  Overall Insight\n",
    "- **High Income + Low Deal Rate → Splurger / Luxury Buyer**  \n",
    "- **Low Income + High Deal Rate → Saver / Bargain Hunter**  \n",
    "- **Moderate Income + Balanced Behavior → Planner / Practical Buyer**\n",
    "\n",
    "These relationships form the foundation for our upcoming **Spending Personality Scoring Rules**, which will classify customers into interpretable segments for the MVP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7cdfd",
   "metadata": {},
   "source": [
    "### Defining Personality Type\n",
    "\n",
    "1. Saver → cautious, deal-seeking, low impulse\n",
    "2. Splurger → high spenders, low restraint\n",
    "3. Planner → balanced, consistent, thoughtful spenders\n",
    "4. Impulse Buyer → spontaneous, high-frequency, high online activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cdbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flags\n",
    "\n",
    "def flag_by_percentile(series, low_q=0.33, high_q=0.67):\n",
    "    low, high = series.quantile([low_q, high_q])\n",
    "    return pd.cut(series, bins=[-float('inf'), low, high, float('inf')], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "\n",
    "df['Income_level'] = flag_by_percentile(df['Income'])\n",
    "df['Spending_level'] = flag_by_percentile(df['TotalMnt'])\n",
    "df['Deal_level'] = flag_by_percentile(df['DealPurchaseRate'])\n",
    "df['Online_level'] = flag_by_percentile(df['WebPurchaseShare'])\n",
    "df['Response_level'] = pd.cut(         #### Beacuse most of the values are zero we use cut instead of quantile(I had an error priror to it)\n",
    "    df['CampaignsAccepted'],\n",
    "    bins=[-np.inf, 0, 1, np.inf],\n",
    "    labels=['Low', 'Mid', 'High'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "\n",
    "# Sanity Check\n",
    "df[['Income_level', 'Spending_level', 'Deal_level', 'Online_level', 'Response_level']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Personalyti Logis\n",
    "\n",
    "def classify_personality(row):\n",
    "    # Saver: deal-seeking, modes income/spend\n",
    "    if row['Deal_level'] == 'High' and row['Income_level'] == 'High':\n",
    "        return 'Saver'  \n",
    "    \n",
    "    # Splurger: high spenders, High Income\n",
    "    elif row['Income_level'] == 'High' and row['Spending_level'] == 'High' and row['Deal_level'] != 'High':\n",
    "        return 'Splurger'\n",
    "    \n",
    "    #Impulse Buyer: high online share, high deal rate\n",
    "    elif row['Online_level'] == 'High' and row['Deal_level'] == 'Low':\n",
    "        return 'Impulse Buyer'\n",
    "    \n",
    "    # Planner: balanced or engaged with campaigns\n",
    "    elif row['Response_level'] == 'High' or (row['Deal_level'] == 'Medium' and row['Online_level'] == 'Medium'):\n",
    "        return 'Planner'\n",
    "    \n",
    "    else: \n",
    "        return 'Planner' # Default to Planner\n",
    "    \n",
    "\n",
    "df['Personality_Type'] = df.apply(classify_personality, axis=1)\n",
    "df['Personality_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49562d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Feature Profiles by Personality Type\n",
    "\n",
    "summary = df.groupby('Personality_Type')[['Income', 'TotalMnt', 'TotalPurchases',\n",
    "                                        'DealPurchaseRate', 'WebPurchaseShare',\n",
    "                                        'AvgMntPerPurchase', 'CampaignsAccepted']].mean().round(2)\n",
    "\n",
    "display(summary)\n",
    "\n",
    "\n",
    "# Visualizing Personality Types\n",
    "summary.plot(kind='bar', subplots=True, layout=(3,3), figsize=(12,10), legend=False, sharex=True)\n",
    "plt.suptitle('Average Feature Values per Personality Type', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4bac8",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e68583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unusable and redundant columns\n",
    "df = df.drop([\"ID\", \"Z_Revenue\",\"Z_CostContact\", \"Marital_Status\", \"Education\"], axis=1)\n",
    "\n",
    "# Checking for missing values\n",
    "df.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing rows with missing values\n",
    "df[df[\"AvgMntPerPurchase\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce242e6",
   "metadata": {},
   "source": [
    "Dropping the 4 rows with missing values as they seem to just add noise. Could be an error in the data entry as never saw so many products at $1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baeb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows\n",
    "df = df[~df[\"AvgMntPerPurchase\"].isna()]\n",
    "\n",
    "# Confirming changes\n",
    "df.isna().sum().sort_values(ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decalring column with hierachical values for ordinal encoding\n",
    "hierarchical_cols = [\"Income_level\",\"Spending_level\",\"Deal_level\",\"Online_level\",\"Response_level\",\"Personality_Type\"]\n",
    "\n",
    "# Ordinal Encoding the hierachical features\n",
    "oe = OrdinalEncoder()\n",
    "df[hierarchical_cols] = oe.fit_transform(df[hierarchical_cols])\n",
    "\n",
    "# Viewing transformations\n",
    "df[hierarchical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b29be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring categorical columns to be One hot Encoded\n",
    "cat_cols = [\"Marital_Status_norm\",\"Education_norm\"]\n",
    "            \n",
    "# OHEncoding categorical features\n",
    "ohe = OneHotEncoder(\n",
    "    handle_unknown=\"error\"\n",
    ")\n",
    "\n",
    "ohe.fit(df[cat_cols])\n",
    "\n",
    "df_ohe = ohe.transform(df[cat_cols]).toarray()\n",
    "\n",
    "# Viewing results\n",
    "df_ohe = pd.DataFrame(df_ohe, columns=ohe.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f21df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining ecnoded features with original dataset\n",
    "df_ohe = pd.concat([df,df_ohe], axis=1)\n",
    "df_ohe = df_ohe.drop([\"Marital_Status_norm\", \"Education_norm\",\"Dt_Customer\"], axis=1)\n",
    "\n",
    "df_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2325351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring numeric features for scaling\n",
    "numeric = list(set(df_ohe.columns) - set(ohe.get_feature_names_out()) - set(hierarchical_cols))\n",
    "\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(f\"Missing values: {df_ohe.isna().sum().sort_values(ascending=False)[0]}\")\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df_ohe = df_ohe.dropna()\n",
    "\n",
    "# Confirming values dropped\n",
    "print(f\"Missing values left: {df_ohe.isna().sum()[0]}\")\n",
    "# Scaling numeric features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transforming numeric features\n",
    "df_scaled = df_ohe.copy()\n",
    "df_scaled[numeric] = scaler.fit_transform(df_ohe[numeric])\n",
    "\n",
    "df_scaled[numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring random seed value\n",
    "state = 42\n",
    "\n",
    "# Clustering data and plotting inertial for optimal number of clusters\n",
    "inertia_scores = []\n",
    "cluster_range = range(3,9)\n",
    "\n",
    "for i in cluster_range:\n",
    "    # print(\"Number of clusters: \",i)\n",
    "    kmeans = KMeans(\n",
    "        n_clusters = i,\n",
    "        n_init = \"auto\",\n",
    "        max_iter = 500,\n",
    "        # verbose = 4,\n",
    "        random_state = state,\n",
    "    )\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting inertia scores from each iteration\n",
    "plt.figure(figsize=(6,9))\n",
    "plt.plot(cluster_range, inertia_scores, marker = 'o')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Check\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fadb18",
   "metadata": {},
   "source": [
    "Using the inertia scores from each iteration we can ideally see an \"elbow\" which would indicate the ideal number of clusters. Being that an elbow has not formed even with 8 clusters, we can conclude that the current dataset as is cannot be properly clustered into a reasonable amount of clusters.\n",
    "\n",
    "We will try a different approach and reduce the dimensioanlity of our dataset into a small number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cea2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels and centroids\n",
    "kmeans_pred = kmeans.predict(df_scaled)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "# Instatiating the PCA tool with 2 principal components\n",
    "pca = PCA(n_components=n_components, random_state=state)\n",
    "\n",
    "# Getting principal components and their values\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Converting results into df\n",
    "df_pca = pd.DataFrame(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23592b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df_pca[0], y=df_pca[1], c=labels, s=100, alpha=0.3)\n",
    "\n",
    "plt.title(\"Principle Component Clustering\")\n",
    "plt.legend(title=\"Cluster\", labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "# Decalaring column names for pc dataframe\n",
    "col_names = [f\"pc_{i+1}\" for i in range(n_components)]\n",
    "\n",
    "# Instantiating PCA \n",
    "pca = PCA(n_components=n_components, random_state=state)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "df_pca = pd.DataFrame(df_pca, columns=col_names)\n",
    "\n",
    "kmeans_pca = KMeans(n_clusters=4)\n",
    "\n",
    "pca_pred = kmeans_pca.fit_predict(df_pca)\n",
    "# pca_pred = pd.DataFrame(pca_pred, columns=df_pca.columns)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['r','g','b','y']\n",
    "cluster_colors = [colors[label] for label in pca_pred]\n",
    "axes3d.Axes3D.scatter(xs=df_pca[\"pc_1\"], ys=df_pca[\"pc_2\"], zs=df_pca[\"pc_3\"], ax=ax, c=cluster_colors)\n",
    "\n",
    "# plt.scatter(cluster_centers[:,0], cluster_centers[:,1], label=\"Cluster Centroids\", marker='*', s=300, c='r')\n",
    "plt.legend(labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f53c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca[\"cluster\"] = pca_pred.astype('str')\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_pca,\n",
    "    x=\"pc_1\",\n",
    "    y=\"pc_2\",\n",
    "    z=\"pc_3\",\n",
    "    color = \"cluster\",\n",
    "    title = \"Interactive 3D Cluster Map\",\n",
    "    opacity = 0.3\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b0201",
   "metadata": {},
   "source": [
    "### Initial Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31218b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting original column names\n",
    "col_names = list(df_scaled.columns)\n",
    "\n",
    "loadings_df = pd.DataFrame(pca.components_.T, columns=[\"pc1\", \"pc_2\", \"pc_3\", \"pc_4\", \"pc_5\"], index=col_names)\n",
    "\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Explained variance by our PCs: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance in 3Pcs: {pca.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085d83b",
   "metadata": {},
   "source": [
    "We're not seeing enough variance within the dataset. We'll increase the number of components and try with larger number of componets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369d9d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
